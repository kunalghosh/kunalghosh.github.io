<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data-science on Kunal&#39;s Blog</title>
    <link>/tags/data-science/</link>
    <description>Kunal&#39;s Blog (data-science)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Jul 2022 00:00:00 +0000</lastBuildDate>
    
    <atom:link href="/tags/data-science/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>GSoC 2022 : Fast Gaussian process implementation in PyMC</title>
      <link>/post/gsoc_first_post/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/post/gsoc_first_post/</guid>
      <description>&lt;p&gt;Gaussian processes (GPs) are very useful class of &lt;code&gt;semi-parametric&lt;/code&gt; machine learning models.
Before their use in more modern classification and regression tasks,
they have been very successfully applied in searching for underground oil fields.
GPs were called &lt;cite&gt;&lt;strong&gt;kriging models&lt;/strong&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/cite&gt; back then, but the idea was the same.&lt;/p&gt;
&lt;p&gt;GPs belong to a general class of models known as &lt;cite&gt; &lt;strong&gt;kernel methods&lt;/strong&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;/cite&gt;.
Kernel methods use something called the &lt;strong&gt;kernel function&lt;/strong&gt;, denoted as&lt;br&gt;
$k(\bold{x},\bold{x&amp;rsquo;})$. Where $\bold{x} \in R^{d}$ represents the input data
and $k$ can be any function which returns a scalar.
For example, the &lt;code&gt;dot-product kernel&lt;/code&gt; $$ k(\bold{x}, \bold{x&amp;rsquo;}) \coloneqq \bold{x}^T\bold{x} $$&lt;/p&gt;
&lt;p&gt;Assuming $N$ such vectors $\bold{x} \in R^{d}$ are stacked, then we can write the input data as
$X \in R^{NxD}$ and correspondingly the kernel is written as $K_{X,X} \in R^{NxN}$.
Let&amp;rsquo;s say, we are interested in building a regression model where the target values
are denoted as $\bold{y} \in R^{n}$ then gaussian process models are trained by optimizing
something called the &lt;cite&gt;&lt;strong&gt;log marginal likelihood&lt;/strong&gt; &lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/cite&gt;.&lt;/p&gt;
&lt;p&gt;Log marginal likelihood is a function of the input $X, \bold{y}$ and is written as,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\tag{1}
L(\theta | X, \bold{y}) \approx log \left| K_{X,X}\right| - \bold{y}^{T}K_{X,X}^{-1}\bold{y}
\end{equation}&lt;/p&gt;
&lt;p&gt;If we want to optimize the above function using gradient based methods we need to
compute the gradient $ \frac{dL}{d\theta} $ which looks like,&lt;/p&gt;
&lt;p&gt;\begin{equation}
\tag{2}
\frac{dL}{d\theta} = \bold{y}^{T} K_{X,X}^{-1} \frac{dK_{X,X}}{d\theta} K_{X,X}^{-1}\bold{y} + \text{Tr} \left( K_{X,X}^{-1} \frac{dK_{X,X}}{d\theta} \right)
\end{equation}&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In equation 1 and 2 above, the most expensive compute steps are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The log determinant : $ log \left| K_{X,X}\right| $&lt;/li&gt;
&lt;li&gt;Inverse of the kernel or compute the &lt;code&gt;solve&lt;/code&gt; : $ K_{X,X}^{-1}\bold{y} $&lt;/li&gt;
&lt;li&gt;Trace : $ \text{Tr} \left( K_{X,X}^{-1} \frac{dK_{X,X}}{d\theta} \right)  $&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;In &lt;cite&gt; Gardner, et.al 2018 &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/cite&gt; they proposed a few algorithms that expresses each of the above three expensive
computations to large matrix computations which can be sped-up when running on a GPU.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For my GSoC, I will implement a sub-class the &lt;a href=&#34;https://github.com/pymc-devs/pymc/blob/562be3781c9d37d3300c4efd4cf6598e5739c32d/pymc/gp/gp.py#L358&#34;&gt;MarginalGP&lt;/a&gt;
and override the &lt;code&gt;_build_conditional()&lt;/code&gt; and &lt;code&gt;_build_marginal_likelihood()&lt;/code&gt; as prescribed in &lt;cite&gt; Gardner, et.al 2018 &lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; &lt;/cite&gt; and that should significantly
speed up Gaussian process inference in PyMC üòç&lt;/p&gt;
&lt;/blockquote&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Kriging&#34;&gt;Wikipedia: Kriging&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;An overview of kernel methods is out of scope of this post, but a good overview of Gaussian processes can be found in &lt;a href=&#34;https://gaussianprocess.org/gpml/&#34;&gt;Rassmussen and Williams&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;&lt;a href=&#34;https://gaussianprocess.org/gpml/chapters/RW2.pdf&#34;&gt;Equation 2.30&lt;/a&gt; in &lt;em&gt;Gaussian Processes for Machine Learning&lt;/em&gt; gives the log marginal likelihood for a zero-mean Gaussian process.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. &lt;a href=&#34;http://arxiv.org/abs/1809.11165&#34;&gt;arxiv&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
