<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>gsoc on Kunal's Blog</title><link>/tags/gsoc/</link><description>Kunal's Blog (gsoc)</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 14 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="/tags/gsoc/index.xml" rel="self" type="application/rss+xml"/><item><title>GSoC 2022 : Fast Gaussian process implementation in PyMC</title><link>/post/gsoc_first_post/</link><pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate><guid>/post/gsoc_first_post/</guid><description>&lt;p>Gaussian processes (GPs) are very useful class of &lt;code>semi-parametric&lt;/code> machine learning models.
Before their use in more modern classification and regression tasks,
they have been very successfully applied in searching for underground oil fields.
GPs were called &lt;cite>&lt;strong>kriging models&lt;/strong> &lt;sup id="fnref:1">&lt;a href="#fn:1" class="footnote-ref" role="doc-noteref">1&lt;/a>&lt;/sup>&lt;/cite> back then, but the idea was the same.&lt;/p>
&lt;p>GPs belong to a general class of models known as &lt;cite> &lt;strong>kernel methods&lt;/strong> &lt;sup id="fnref:2">&lt;a href="#fn:2" class="footnote-ref" role="doc-noteref">2&lt;/a>&lt;/sup> &lt;/cite>.
Kernel methods use something called the &lt;strong>kernel function&lt;/strong>, denoted as&lt;br>
$k(\bold{x},\bold{x&amp;rsquo;})$. Where $\bold{x} \in R^{d}$ represents the input data
and $k$ can be any function which returns a scalar.
For example, the &lt;code>dot-product kernel&lt;/code> $$ k(\bold{x}, \bold{x&amp;rsquo;}) \coloneqq \bold{x}^T\bold{x} $$&lt;/p>
&lt;p>Assuming $N$ such vectors $\bold{x} \in R^{d}$ are stacked, then we can write the input data as
$X \in R^{NxD}$ and correspondingly the kernel is written as $K_{X,X} \in R^{NxN}$.
Let&amp;rsquo;s say, we are interested in building a regression model where the target values
are denoted as $\bold{y} \in R^{n}$ then gaussian process models are trained by optimizing
something called the &lt;cite>&lt;strong>log marginal likelihood&lt;/strong> &lt;sup id="fnref:3">&lt;a href="#fn:3" class="footnote-ref" role="doc-noteref">3&lt;/a>&lt;/sup>&lt;/cite>.&lt;/p>
&lt;p>Log marginal likelihood is a function of the input $X, \bold{y}$ and is written as,&lt;/p>
&lt;p>\begin{equation}
\tag{1}
L(\theta | X, \bold{y}) \approx log \left| K_{X,X}\right| - \bold{y}^{T}K_{X,X}^{-1}\bold{y}
\end{equation}&lt;/p>
&lt;p>If we want to optimize the above function using gradient based methods we need to
compute the gradient $ \frac{dL}{d\theta} $ which looks like,&lt;/p>
&lt;p>\begin{equation}
\tag{2}
\frac{dL}{d\theta} = \bold{y}^{T} K_{X,X}^{-1} \frac{dK_{X,X}}{d\theta} K_{X,X}^{-1}\bold{y} + \text{Tr} \left( K_{X,X}^{-1} \frac{dK_{X,X}}{d\theta} \right)
\end{equation}&lt;/p>
&lt;blockquote>
&lt;p>In equation 1 and 2 above, the most expensive compute steps are&lt;/p>
&lt;ol>
&lt;li>The log determinant : $ log \left| K_{X,X}\right| $&lt;/li>
&lt;li>Inverse of the kernel or compute the &lt;code>solve&lt;/code> : $ K_{X,X}^{-1}\bold{y} $&lt;/li>
&lt;li>Trace : $ \text{Tr} \left( K_{X,X}^{-1} \frac{dK_{X,X}}{d\theta} \right) $&lt;/li>
&lt;/ol>
&lt;/blockquote>
&lt;p>In &lt;cite> Gardner, et.al 2018 &lt;sup id="fnref:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> &lt;/cite> they proposed a few algorithms that expresses each of the above three expensive
computations to large matrix computations which can be sped-up when running on a GPU.&lt;/p>
&lt;blockquote>
&lt;p>For my GSoC, I will sub-class &lt;a href="https://github.com/pymc-devs/pymc/blob/562be3781c9d37d3300c4efd4cf6598e5739c32d/pymc/gp/gp.py#L358">MarginalGP&lt;/a>
and override the &lt;code>_build_conditional()&lt;/code> and &lt;code>_build_marginal_likelihood()&lt;/code> as prescribed in &lt;cite> Gardner, et.al 2018 &lt;sup id="fnref1:4">&lt;a href="#fn:4" class="footnote-ref" role="doc-noteref">4&lt;/a>&lt;/sup> &lt;/cite> and that should significantly
speed up Gaussian process inference in PyMC 😍&lt;/p>
&lt;/blockquote>
&lt;div class="footnotes" role="doc-endnotes">
&lt;hr>
&lt;ol>
&lt;li id="fn:1">
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Kriging">Wikipedia: Kriging&lt;/a>&amp;#160;&lt;a href="#fnref:1" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:2">
&lt;p>An overview of kernel methods is out of scope of this post, but a good overview of Gaussian processes can be found in &lt;a href="https://gaussianprocess.org/gpml/">Rassmussen and Williams&lt;/a>.&amp;#160;&lt;a href="#fnref:2" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:3">
&lt;p>&lt;a href="https://gaussianprocess.org/gpml/chapters/RW2.pdf">Equation 2.30&lt;/a> in &lt;em>Gaussian Processes for Machine Learning&lt;/em> gives the log marginal likelihood for a zero-mean Gaussian process.&amp;#160;&lt;a href="#fnref:3" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;li id="fn:4">
&lt;p>GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. &lt;a href="http://arxiv.org/abs/1809.11165">arxiv&lt;/a>&amp;#160;&lt;a href="#fnref:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&amp;#160;&lt;a href="#fnref1:4" class="footnote-backref" role="doc-backlink">&amp;#x21a9;&amp;#xfe0e;&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;/div></description></item><item><title>GSoC 2022 : Final Report</title><link>/post/gsoc_final_post/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/post/gsoc_final_post/</guid><description>&lt;h2 id="gsoc-ends-but-the-contribution-continues" >GSoC ends but the contribution continues
&lt;span>
&lt;a href="#gsoc-ends-but-the-contribution-continues">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h2>&lt;p>This GSoC has been a great experience for me. Partly because of the technical aspects of the project,
I loved coding the algorithms and contributing to an open-source project felt refreshing !
But most importantly because of my mentors &lt;a href="https://github.com/fonnesbeck">Chris Fonnesbeck&lt;/a> and &lt;a href="https://github.com/bwengals">Bill Engels&lt;/a>.
Their feedback was timely and precise, thank you ! I also found the PyMC communitiny to be very helpful, considerate and welcoming !
I plan to continue working on my project and beyond, and can highly recomment this project to future contributors.&lt;/p>
&lt;h1 id="status-update" >Status update
&lt;span>
&lt;a href="#status-update">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>The objective of my Google summer of code project was to speed up Gaussian process inference in PyMC by
leveraging Fast Matrix-Matrix multiplication based algorithms to speed up the
&lt;code>log-determinant&lt;/code> and &lt;code>solve&lt;/code> computations which are the most expensive computations during GP inference.&lt;/p>
&lt;p>The project was mainly divided into the the following parts:&lt;/p>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Familiarise myself with PyMC Codebase
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Come up with a plan to implement the algorithms&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Finish initial implementations in NumPy of:&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> pivoted Cholesky Algorithm.&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Linear Conjugate Gradients, which works on batches of data.&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Check-in above two implementations.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Change the algorithms to use JAX instead, check GPU utilisation.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Implement log-determinant and solve functions.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Implement a GP using the GPU accelerated log-determinant and solve functions.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Convert Log-determinant and solve functions to Aesara OPs.&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Integrate code into PyMC&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The list above also indicates the status of the individual milestones. I plan to continue working on the unfinished items after GSoC.&lt;/p>
&lt;h1 id="contributions" >Contributions
&lt;span>
&lt;a href="#contributions">
&lt;svg viewBox="0 0 28 23" height="100%" width="19" xmlns="http://www.w3.org/2000/svg">&lt;path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2"/>&lt;/svg>
&lt;/a>
&lt;/span>
&lt;/h1>&lt;p>As a part of my GSoC, I sent two pull-requests which will be merged soon &lt;a href="https://github.com/pymc-devs/pymc-experimental/pull/62">PR:62&lt;/a> &lt;a href="https://github.com/pymc-devs/pymc-experimental/pull/63">PR:63&lt;/a>
which implement the core logic of pivoted Cholesky algorithm and the batched linear conjugate gradients, in NumPy. Apart from forming a good starting point for moving on to JAX for GPU speed-up.
The NumPy impementations are also readable reference implementations for someone looking to working on something similar in the future.&lt;/p>
&lt;p>I also maintained a log of my work on &lt;a href="https://hackmd.io/@CblWjfoIRO2tmCH8-j2AZA/HJTP7aPO9">HackMD&lt;/a> and wrote a few blog posts &lt;a href="https://kunalghosh.github.io/tags/gsoc/">here&lt;/a>.
I really like the process of maintaining a work log, it helps to give a sense of progres to the project and also helps to
keep track of hacks and work-arounds which one tends to forget. I can highly recommend keeping a digital work diary for your projects.&lt;/p></description></item></channel></rss>